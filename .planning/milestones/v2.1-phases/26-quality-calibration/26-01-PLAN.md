---
phase: 26-quality-calibration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - lib/quality-config.ts
  - lib/workflow-engine.ts
  - lib/research-executor.ts
autonomous: false
requirements: [QUAL-01]

must_haves:
  truths:
    - 'Admin sees a terminal review table: one row per prospect with name, source type count, current score, proposed tier'
    - 'Threshold constants live in lib/quality-config.ts with comments explaining their semantic meaning'
    - 'computeTrafficLight in lib/workflow-engine.ts reads thresholds from quality-config.ts constants'
    - 'Re-running research deletes old hypotheses before inserting new ones (no duplicate rows)'
    - 'Admin approves proposed thresholds and they are committed to the codebase'
  artifacts:
    - path: 'lib/quality-config.ts'
      provides: 'Threshold constants AMBER_MIN_SOURCE_TYPES, GREEN_MIN_SOURCE_TYPES, MIN_EVIDENCE_COUNT with semantic comments'
      min_lines: 20
    - path: 'lib/workflow-engine.ts'
      provides: 'computeTrafficLight reads from quality-config.ts constants'
      contains: 'AMBER_MIN_SOURCE_TYPES'
    - path: 'lib/research-executor.ts'
      provides: 'executeResearchRun deletes existing hypotheses before inserting new ones'
      contains: 'deleteMany'
  key_links:
    - from: 'lib/workflow-engine.ts'
      to: 'lib/quality-config.ts'
      via: 'import of threshold constants'
      pattern: 'AMBER_MIN_SOURCE_TYPES|GREEN_MIN_SOURCE_TYPES'
    - from: 'lib/research-executor.ts'
      to: 'prisma.workflowHypothesis.deleteMany'
      via: 'delete-before-insert in executeResearchRun'
      pattern: 'deleteMany.*researchRunId'
---

<objective>
Calibrate amber/green quality thresholds against real Dutch marketing agency data and fix the hypothesis re-run idempotency bug.

Purpose: The current thresholds (sourceTypeCount < 2 → amber) were set with estimated defaults, not real data. AMBER semantics also changed: it now blocks send until admin reviews (not soft warn-and-proceed). A calibration script must inspect all real prospects and propose threshold values before they are committed.

Output:

- lib/quality-config.ts — threshold constants with semantic comments
- lib/workflow-engine.ts — computeTrafficLight updated to use config constants, comment updated to reflect amber-blocks behavior
- lib/research-executor.ts — delete-before-insert fix for hypothesis idempotency
- Terminal review table showing per-prospect quality breakdown under proposed thresholds
- Human-approved threshold values committed to codebase
  </objective>

<execution_context>
@/home/klarifai/.claude/get-shit-done/workflows/execute-plan.md
@/home/klarifai/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/26-quality-calibration/26-CONTEXT.md

Key existing code to understand before implementing:

- lib/workflow-engine.ts — computeTrafficLight (line ~516), evaluateQualityGate (line ~469), runSummaryPayload (line ~1404)
- lib/research-executor.ts — executeResearchRun, the point where hypotheses are inserted
- server/routers/admin.ts — listProspects query shape (line ~427)
- .planning/phases/25-pipeline-hardening/25-03-NOTES.md — per-prospect hypothesis quality findings (provides evidence on what each prospect's source diversity actually is)

Design decisions (locked from 26-CONTEXT.md):

- PRIMARY quality signal: source type diversity (how many distinct sourceType values contributed evidence)
- GREEN = multiple strong evidence types (CONTEXT says 2-3+ distinct source types)
- AMBER = evidence found but fewer source types — blocks send until admin explicitly reviews
- RED = hard block — too low to send at all
- Threshold storage: constants in config file, NOT .env, NOT DB
- Comments required explaining semantic meaning
- No per-prospect tier stored in DB — computed dynamically at render time
  </context>

<tasks>

<task type="auto">
  <name>Task 1: Create quality-config.ts and update computeTrafficLight + hypothesis idempotency</name>
  <files>
    lib/quality-config.ts
    lib/workflow-engine.ts
    lib/research-executor.ts
  </files>
  <action>
**1. Create lib/quality-config.ts** — threshold constants with semantic comments.

Use source-diversity as the primary signal per 26-CONTEXT.md decisions. Start with these initial values (calibration in Task 2 will determine if they need adjustment):

```ts
/**
 * Quality calibration thresholds for the Qualifai research pipeline.
 *
 * Primary signal: source type diversity — how many distinct sourceType values
 * (homepage, google, sitemap, kvk, linkedin, etc.) contributed evidence items.
 *
 * Tier semantics:
 *   GREEN  = 3+ source types — strong multi-source evidence, ready to outreach
 *   AMBER  = 2 source types — some evidence but limited diversity — BLOCKS send
 *             until admin explicitly reviews and approves
 *   RED    = 0-1 source types or < MIN_EVIDENCE_COUNT items — too thin to proceed
 *
 * Note: AMBER is a HARD gate (not soft warn-and-proceed). The send queue will
 * not allow outreach for AMBER prospects unless qualityApproved === true.
 *
 * Calibrated: 2026-02-27 against 5 real Dutch marketing agency prospects.
 * See: .planning/phases/26-quality-calibration/26-01-SUMMARY.md for calibration data.
 */

/** Minimum number of evidence items required to avoid RED */
export const MIN_EVIDENCE_COUNT = 3;

/** Source type count threshold for AMBER (1 source type → RED; 2 → AMBER; 3+ → GREEN) */
export const AMBER_MIN_SOURCE_TYPES = 2;

/** Source type count threshold for GREEN (must meet or exceed this to be GREEN) */
export const GREEN_MIN_SOURCE_TYPES = 3;

/** Minimum average confidence score (secondary signal, used when source types alone are borderline) */
export const MIN_AVERAGE_CONFIDENCE = 0.65;
```

**2. Update lib/workflow-engine.ts — computeTrafficLight**

Import the constants from quality-config.ts and rewrite computeTrafficLight to use them:

```ts
import {
  MIN_EVIDENCE_COUNT,
  AMBER_MIN_SOURCE_TYPES,
  GREEN_MIN_SOURCE_TYPES,
  MIN_AVERAGE_CONFIDENCE,
} from '@/lib/quality-config';
```

Update the function and its JSDoc comment (remove "amber never blocks" — it now blocks):

```ts
/**
 * computeTrafficLight — pure function mapping evidence metrics to a traffic light.
 *
 * Red:   critically thin evidence (< MIN_EVIDENCE_COUNT items OR 0 source types)
 * Amber: limited source diversity (sourceTypeCount < GREEN_MIN_SOURCE_TYPES) —
 *        BLOCKS send until admin explicitly reviews and approves
 * Green: strong multi-source evidence (sourceTypeCount >= GREEN_MIN_SOURCE_TYPES)
 *
 * Primary signal: source type diversity.
 * Thresholds defined in lib/quality-config.ts.
 */
export function computeTrafficLight(
  evidenceCount: number,
  sourceTypeCount: number,
  averageConfidence: number,
): TrafficLight {
  if (evidenceCount < MIN_EVIDENCE_COUNT || sourceTypeCount < 1) return 'red';
  if (
    sourceTypeCount < GREEN_MIN_SOURCE_TYPES ||
    averageConfidence < MIN_AVERAGE_CONFIDENCE
  )
    return 'amber';
  return 'green';
}
```

Note: AMBER_MIN_SOURCE_TYPES imported but used as the lower bound (1 type → red, 2 types → amber, 3+ types → green). The constant documents the value even if the logic uses it implicitly via the green threshold.

**3. Update lib/research-executor.ts — delete-before-insert for hypothesis idempotency**

Find the section in `executeResearchRun` where `workflowHypothesis.createMany` is called (hypothesis insertion). Before the createMany, add a deleteMany to clear existing hypotheses for this run:

```ts
// Clear existing hypotheses before inserting new ones — ensures re-runs are idempotent
await ctx.db.workflowHypothesis.deleteMany({
  where: { researchRunId: run.id },
});
```

Place this immediately before the `workflowHypothesis.createMany` call. This fixes the Phase 25-03 deferred bug where old construction templates co-existed with new AI-generated hypotheses.

After making all three changes, run:

```
cd /home/klarifai/Documents/klarifai/projects/qualifai && npm run check
```

Fix all TypeScript and lint errors before proceeding.
</action>
<verify>
<automated>cd /home/klarifai/Documents/klarifai/projects/qualifai && npm run check</automated>
<manual>Confirm lib/quality-config.ts exists with MIN_EVIDENCE_COUNT, AMBER_MIN_SOURCE_TYPES, GREEN_MIN_SOURCE_TYPES constants. Confirm computeTrafficLight imports from quality-config. Confirm research-executor.ts has deleteMany before createMany for hypotheses.</manual>
</verify>
<done> - lib/quality-config.ts exists with 4 exported constants and semantic comments - computeTrafficLight in workflow-engine.ts imports and uses GREEN_MIN_SOURCE_TYPES - research-executor.ts has workflowHypothesis.deleteMany({ where: { researchRunId: run.id } }) before createMany - npm run check passes with zero errors
</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 2: Run calibration script and approve thresholds</name>
  <files>lib/quality-config.ts</files>
  <action>
    Run the calibration query below to produce the review table, then adjust lib/quality-config.ts constants if the distribution is wrong. No automated code changes — this is a read-and-decide step.
  </action>
  <what-built>
    Task 1 created quality-config.ts with initial threshold values (GREEN_MIN_SOURCE_TYPES=3, MIN_EVIDENCE_COUNT=3).
    Now run the calibration check to see how real prospects classify under these thresholds.
  </what-built>
  <how-to-verify>
    Run this calibration query to inspect real prospect quality data:

    ```bash
    cd /home/klarifai/Documents/klarifai/projects/qualifai
    node -e "
    require('dotenv').config({ path: '.env.local' });
    const { PrismaClient } = require('@prisma/client');
    const { PrismaPg } = require('@prisma/adapter-pg');
    const { Pool } = require('pg');
    const pool = new Pool({ connectionString: process.env.DATABASE_URL });
    const adapter = new PrismaPg(pool);
    const db = new PrismaClient({ adapter });

    async function main() {
      const prospects = await db.prospect.findMany({
        include: {
          researchRuns: {
            orderBy: { createdAt: 'desc' },
            take: 1,
            include: {
              evidenceItems: { select: { sourceType: true, confidenceScore: true } }
            }
          }
        }
      });

      console.log('\\n--- QUALITY CALIBRATION TABLE ---\\n');
      console.log('Prospect'.padEnd(35) + 'EvidCt'.padEnd(8) + 'SrcTypes'.padEnd(10) + 'AvgConf'.padEnd(10) + 'Tier (3=green)');
      console.log('-'.repeat(80));

      for (const p of prospects) {
        const run = p.researchRuns[0];
        if (!run) {
          console.log((p.companyName ?? p.domain ?? 'unknown').padEnd(35) + 'No data');
          continue;
        }
        const items = run.evidenceItems;
        const evidCt = items.length;
        const srcTypes = new Set(items.map(e => e.sourceType)).size;
        const avgConf = items.length > 0 ? (items.reduce((s, e) => s + e.confidenceScore, 0) / items.length).toFixed(2) : '0';
        const tier = evidCt < 3 || srcTypes < 1 ? 'RED' : srcTypes < 3 || parseFloat(avgConf) < 0.65 ? 'AMBER' : 'GREEN';
        console.log((p.companyName ?? p.domain ?? 'unknown').padEnd(35) + String(evidCt).padEnd(8) + String(srcTypes).padEnd(10) + String(avgConf).padEnd(10) + tier);
      }
      console.log('\\n');
      await db.\$disconnect();
    }
    main().catch(e => { console.error(e); process.exit(1); });
    "
    ```

    Review the output table. Expected distribution: mostly AMBER with 1-2 GREEN.

    If all prospects are RED: lower MIN_EVIDENCE_COUNT in lib/quality-config.ts
    If all are GREEN: raise GREEN_MIN_SOURCE_TYPES or lower MIN_AVERAGE_CONFIDENCE
    If distribution looks reasonable (mix of AMBER/GREEN with maybe 1-2 RED): thresholds are good

    Adjust lib/quality-config.ts constants if needed, then type "approved" to proceed.

  </how-to-verify>
  <verify>
    <automated>MISSING — human must run calibration script and review output table</automated>
    <manual>Run calibration query above, review table, confirm distribution matches expected (mostly AMBER with 1-2 GREEN). Adjust lib/quality-config.ts if needed.</manual>
  </verify>
  <done>Admin has reviewed calibration table, confirmed or adjusted threshold constants, and approved the distribution.</done>
  <resume-signal>Type "approved" after reviewing the table and confirming threshold values in lib/quality-config.ts are correct for the observed distribution.</resume-signal>
</task>

</tasks>

<verification>
1. lib/quality-config.ts exists: `ls lib/quality-config.ts` — should return the file
2. Constants imported in workflow-engine.ts: `grep "quality-config" lib/workflow-engine.ts` — should find import
3. Hypothesis idempotency: `grep "deleteMany.*researchRunId\|researchRunId.*deleteMany" lib/research-executor.ts` — should find delete call
4. Types pass: `npm run check` — zero errors
5. Calibration table reviewed and thresholds approved by admin
</verification>

<success_criteria>

- lib/quality-config.ts exists with threshold constants and semantic comments
- computeTrafficLight in workflow-engine.ts uses GREEN_MIN_SOURCE_TYPES (not hardcoded 3)
- Re-running research no longer produces duplicate hypothesis rows
- Admin has reviewed calibration table and confirmed thresholds reflect real prospect distribution
- npm run check passes
  </success_criteria>

<output>
After completion, create `.planning/phases/26-quality-calibration/26-01-SUMMARY.md` including:
- What constants were set and why (from calibration table observation)
- Per-prospect tier breakdown table from the calibration script
- Hypothesis idempotency fix details
- Commit hashes
</output>
