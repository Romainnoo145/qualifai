---
phase: 08-deep-evidence-pipeline
plan: 02
type: tdd
wave: 1
depends_on: []
files_modified:
  - lib/enrichment/crawl4ai.ts
  - lib/enrichment/crawl4ai.test.ts
autonomous: true

must_haves:
  truths:
    - 'extractMarkdown fetches page content from Crawl4AI REST API and returns markdown + title'
    - 'Crawl4AI request body uses the required {type: ClassName, params: {...}} wrapping format'
    - 'Cookie consent is handled via magic: true and simulate_user: true parameters'
    - 'ingestCrawl4aiEvidenceDrafts returns EvidenceDraft[] with correct sourceType (REVIEWS for review URLs, JOB_BOARD for others)'
    - 'Pages that return empty or minimal content produce fallback drafts instead of being silently dropped'
    - 'URLs are capped at 10 per batch to prevent runaway extraction'
    - 'Crawl4AI timeouts (60s) do not crash the caller -- empty result returned'
  artifacts:
    - path: 'lib/enrichment/crawl4ai.ts'
      provides: 'Crawl4AI REST API client and evidence draft ingestion'
      exports: ['extractMarkdown', 'ingestCrawl4aiEvidenceDrafts']
    - path: 'lib/enrichment/crawl4ai.test.ts'
      provides: 'Unit tests for Crawl4AI client with mocked fetch'
      contains: 'ingestCrawl4aiEvidenceDrafts'
  key_links:
    - from: 'lib/enrichment/crawl4ai.ts'
      to: 'Crawl4AI Docker API'
      via: 'fetch POST to CRAWL4AI_BASE_URL/crawl'
      pattern: 'fetch.*crawl'
    - from: 'lib/enrichment/crawl4ai.ts'
      to: 'lib/workflow-engine.ts'
      via: 'EvidenceDraft type import'
      pattern: 'import.*EvidenceDraft.*workflow-engine'
---

<objective>
Create the Crawl4AI REST API client that extracts browser-rendered page content as markdown and converts it to EvidenceDraft objects.

Purpose: Crawl4AI is the extraction layer -- it takes URLs discovered by SerpAPI (plan 08-01) and extracts their content using a real browser. This handles JS-rendered pages and cookie consent banners that plain fetch cannot.

Output: `lib/enrichment/crawl4ai.ts` exporting `extractMarkdown()` and `ingestCrawl4aiEvidenceDrafts()` with TDD tests.
</objective>

<execution_context>
@/home/klarifai/.claude/get-shit-done/workflows/execute-plan.md
@/home/klarifai/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-deep-evidence-pipeline/08-RESEARCH.md

# Existing adapter patterns to follow

@lib/web-evidence-adapter.ts
@lib/review-adapters.ts

# EvidenceDraft interface

@lib/workflow-engine.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Crawl4AI client with TDD</name>
  <files>lib/enrichment/crawl4ai.ts, lib/enrichment/crawl4ai.test.ts</files>
  <action>
**RED phase: Write tests first in `lib/enrichment/crawl4ai.test.ts`.**

Mock global `fetch` using `vi.fn()` to intercept calls to the Crawl4AI REST API.

Test cases for `extractMarkdown`:

1. **Successful extraction** -- Mock fetch to return `{ success: true, results: [{ markdown: '# Page Title\nContent here', metadata: { title: 'Page Title' } }] }`. Verify returns `{ markdown: '# Page Title\nContent here', title: 'Page Title' }`.
2. **Empty markdown response** -- Mock fetch to return `{ success: true, results: [{ markdown: '' }] }`. Verify returns `{ markdown: '', title: '' }`.
3. **Fetch failure (non-200)** -- Mock fetch to return `{ ok: false, status: 500 }`. Verify returns `{ markdown: '', title: '' }` without throwing.
4. **Network timeout** -- Mock fetch to throw `AbortError`. Verify returns `{ markdown: '', title: '' }` without throwing.
5. **Request body format** -- Capture the fetch call arguments. Verify the body contains `browser_config: { type: 'BrowserConfig', params: { headless: true } }` and `crawler_config: { type: 'CrawlerRunConfig', params: { cache_mode: 'bypass', magic: true, simulate_user: true, wait_for_timeout: 15000, delay_before_return_html: 2 } }`.

Test cases for `ingestCrawl4aiEvidenceDrafts`: 6. **Review URL produces REVIEWS sourceType** -- Pass a URL containing `google.com/maps`. Verify the EvidenceDraft has `sourceType: 'REVIEWS'`. 7. **Non-review URL produces JOB_BOARD sourceType** -- Pass a URL like `https://example.com/jobs/dev`. Verify `sourceType: 'JOB_BOARD'`. 8. **Fallback draft for minimal content** -- Mock extractMarkdown to return markdown shorter than 80 chars. Verify a fallback draft is created with `confidenceScore: 0.55` and `metadata.fallback: true`. 9. **URL cap at 10** -- Pass 15 URLs. Verify only 10 are processed (check fetch call count). 10. **Snippet truncation** -- Mock extractMarkdown to return 500-char markdown. Verify snippet is truncated to ~240 chars with newlines replaced.

**GREEN phase: Implement `lib/enrichment/crawl4ai.ts`.**

Use `process.env.CRAWL4AI_BASE_URL ?? 'http://localhost:11235'` for the base URL (not env.mjs import -- keeps module independently testable, same reasoning as serp.ts).

Internal function `extractMarkdown(url: string): Promise<{ markdown: string; title: string }>`:

- Build request body with the **critical** `{ type: 'ClassName', params: {...} }` wrapping:
  ```
  {
    urls: [url],
    browser_config: { type: 'BrowserConfig', params: { headless: true } },
    crawler_config: {
      type: 'CrawlerRunConfig',
      params: {
        cache_mode: 'bypass',
        magic: true,
        simulate_user: true,
        wait_for_timeout: 15000,
        delay_before_return_html: 2,
      },
    },
  }
  ```
- POST to `${CRAWL4AI_BASE_URL}/crawl` with Content-Type application/json
- 60-second timeout using AbortController (create controller, setTimeout to abort at 60000ms, clear on response)
- Parse response as JSON. Extract `results[0].markdown` and `results[0].metadata.title`.
- On ANY error (fetch failure, timeout, non-OK response, JSON parse error): return `{ markdown: '', title: '' }`. Never throw.

Exported function `ingestCrawl4aiEvidenceDrafts(urls: string[]): Promise<EvidenceDraft[]>`:

- Process at most 10 URLs: `urls.slice(0, 10)`
- For each URL, call `extractMarkdown(url)`
- If markdown is empty or < 80 chars: create fallback draft (same pattern as `fallbackDraft` in web-evidence-adapter.ts):
  ```
  {
    sourceType: 'REVIEWS',
    sourceUrl: url,
    title: title || 'Source (browser extraction)',
    snippet: 'Page queued for manual review -- browser extraction returned minimal content.',
    workflowTag: 'workflow-context',
    confidenceScore: 0.55,
    metadata: { adapter: 'crawl4ai', fallback: true },
  }
  ```
- If markdown is sufficient: create real draft:
  ```
  {
    sourceType: url.includes('google.com/maps') ? 'REVIEWS' : 'JOB_BOARD',
    sourceUrl: url,
    title: title || 'Browser-extracted page',
    snippet: markdown.slice(0, 240).replace(/\n+/g, ' ').trim(),
    workflowTag: 'workflow-context',
    confidenceScore: 0.76,
    metadata: { adapter: 'crawl4ai', source: 'serp-discovery' },
  }
  ```
- Import `EvidenceDraft` type from `@/lib/workflow-engine`.
- Export both `extractMarkdown` (for testing) and `ingestCrawl4aiEvidenceDrafts` (for pipeline).

**REFACTOR phase:** Ensure types are tight, no `any` usage.

Run `npx vitest run lib/enrichment/crawl4ai.test.ts` -- all tests pass.
Run `npm run check` -- no lint/type errors.
</action>
<verify>`npx vitest run lib/enrichment/crawl4ai.test.ts` -- all 10 test cases pass. `npm run check` passes.</verify>
<done>extractMarkdown calls Crawl4AI REST API with correct wrapped body format, handles timeouts and errors gracefully, returns markdown + title. ingestCrawl4aiEvidenceDrafts converts extracted content to EvidenceDraft[] with correct sourceType, fallback for minimal content, URL cap at 10, and snippet truncation.</done>
</task>

</tasks>

<verification>
- `npx vitest run lib/enrichment/crawl4ai.test.ts` -- all tests pass
- `npm run check` -- no type or lint errors
- Request body uses `{ type: 'BrowserConfig', params: {...} }` wrapping (verified by test case 5)
- `magic: true` and `simulate_user: true` present in crawler_config params
- Fallback drafts created for pages with < 80 chars markdown
- URLs capped at 10 per batch
</verification>

<success_criteria>

1. `extractMarkdown('https://example.com')` returns `{ markdown, title }` from Crawl4AI API
2. Network errors and timeouts return empty results without throwing
3. `ingestCrawl4aiEvidenceDrafts(['https://google.com/maps/place/...'])` returns EvidenceDraft with `sourceType: 'REVIEWS'`
4. Fallback drafts created for empty/minimal extraction results
5. All tests pass, no lint/type errors
   </success_criteria>

<output>
After completion, create `.planning/phases/08-deep-evidence-pipeline/08-02-SUMMARY.md`
</output>
