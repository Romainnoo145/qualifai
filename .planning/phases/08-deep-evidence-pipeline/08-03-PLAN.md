---
phase: 08-deep-evidence-pipeline
plan: 03
type: execute
wave: 2
depends_on: ['08-01', '08-02']
files_modified:
  - lib/research-executor.ts
  - server/routers/research.ts
autonomous: true

must_haves:
  truths:
    - 'startRun mutation accepts optional deepCrawl boolean flag'
    - 'When deepCrawl is true, executeResearchRun calls discoverSerpUrls and ingestCrawl4aiEvidenceDrafts'
    - 'When deepCrawl is false or absent, research runs exactly as before with no SerpAPI or Crawl4AI calls'
    - 'SerpAPI cache is persisted in ResearchRun.inputSnapshot and read on retry/re-run within 24 hours'
    - 'SERP-discovered evidence drafts are deduped with existing drafts before persisting'
    - 'Deep crawl evidence flows through the existing approval gate (Phase 7) without bypass'
  artifacts:
    - path: 'lib/research-executor.ts'
      provides: 'executeResearchRun with deepCrawl branch'
      contains: 'deepCrawl'
    - path: 'server/routers/research.ts'
      provides: 'startRun input with deepCrawl flag'
      contains: 'deepCrawl'
  key_links:
    - from: 'lib/research-executor.ts'
      to: 'lib/enrichment/serp.ts'
      via: 'discoverSerpUrls import and call'
      pattern: 'import.*discoverSerpUrls.*serp'
    - from: 'lib/research-executor.ts'
      to: 'lib/enrichment/crawl4ai.ts'
      via: 'ingestCrawl4aiEvidenceDrafts import and call'
      pattern: 'import.*ingestCrawl4aiEvidenceDrafts.*crawl4ai'
    - from: 'server/routers/research.ts'
      to: 'lib/research-executor.ts'
      via: 'deepCrawl flag passed through to executeResearchRun'
      pattern: "deepCrawl.*input\\.deepCrawl"
---

<objective>
Wire the SerpAPI discovery client and Crawl4AI extraction client into the existing research pipeline so that `deepCrawl: true` triggers the full deep evidence flow.

Purpose: This is the integration plan that connects the two new adapters (08-01, 08-02) into the existing `executeResearchRun` function and `research.startRun` tRPC mutation. Without this wiring, the clients exist but are never called.

Output: Modified `research-executor.ts` and `server/routers/research.ts` with deepCrawl support and SerpAPI result caching.
</objective>

<execution_context>
@/home/klarifai/.claude/get-shit-done/workflows/execute-plan.md
@/home/klarifai/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-deep-evidence-pipeline/08-RESEARCH.md
@.planning/phases/08-deep-evidence-pipeline/08-01-SUMMARY.md
@.planning/phases/08-deep-evidence-pipeline/08-02-SUMMARY.md

# Files to modify

@lib/research-executor.ts
@server/routers/research.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add deepCrawl flag to tRPC mutation and executor input</name>
  <files>server/routers/research.ts, lib/research-executor.ts</files>
  <action>
**In `server/routers/research.ts`:**

Add `deepCrawl: z.boolean().default(false)` to the `startRun` input schema (alongside `prospectId`, `campaignId`, `manualUrls`).

Pass `deepCrawl: input.deepCrawl` through to `executeResearchRun` in the startRun mutation.

In the `retryRun` mutation, read the `deepCrawl` value from the existing run's `inputSnapshot`:

```typescript
const snapshot = existing.inputSnapshot as Record<string, unknown> | null;
const deepCrawl = snapshot?.deepCrawl === true;
```

Pass `deepCrawl` through to `executeResearchRun` in retryRun as well.

**In `lib/research-executor.ts`:**

Add `deepCrawl?: boolean` to the `executeResearchRun` input type.

Add two new imports at the top:

```typescript
import {
  discoverSerpUrls,
  type SerpDiscoveryResult,
} from '@/lib/enrichment/serp';
import { ingestCrawl4aiEvidenceDrafts } from '@/lib/enrichment/crawl4ai';
```

Persist `deepCrawl` flag in `inputSnapshot` so retryRun can read it:

```typescript
inputSnapshot: toJson({
  manualUrls: input.manualUrls,
  campaignId: input.campaignId,
  deepCrawl: input.deepCrawl ?? false,
}),
```

Apply this to BOTH the create and update paths for the `run` variable.
</action>
<verify>`npm run check` passes -- no type errors in the modified files. The deepCrawl flag is declared in the input schema and passed through without being consumed yet (next task wires the actual calls).</verify>
<done>deepCrawl boolean accepted by startRun and retryRun mutations, forwarded to executeResearchRun, persisted in inputSnapshot.</done>
</task>

<task type="auto">
  <name>Task 2: Wire SerpAPI discovery and Crawl4AI extraction into executeResearchRun</name>
  <files>lib/research-executor.ts</files>
  <action>
Add a helper function to extract SERP cache from inputSnapshot:

```typescript
function extractSerpCache(snapshot: unknown): SerpDiscoveryResult | null {
  const payload = snapshot as { serpCache?: SerpDiscoveryResult } | null;
  if (!payload?.serpCache?.discoveredAt) return null;
  return payload.serpCache;
}
```

In `executeResearchRun`, AFTER the existing `reviewEvidenceDrafts` block and BEFORE the `dedupeEvidenceDrafts` call, add the deep crawl branch:

```typescript
if (input.deepCrawl) {
  // Check cache (24-hour TTL)
  const existingSnapshot = input.existingRunId
    ? (
        await db.researchRun.findUnique({
          where: { id: input.existingRunId },
          select: { inputSnapshot: true },
        })
      )?.inputSnapshot
    : null;
  const serpCache = extractSerpCache(existingSnapshot);
  const isCacheValid =
    serpCache &&
    Date.now() - new Date(serpCache.discoveredAt).getTime() <
      24 * 60 * 60 * 1000;

  const serpResult: SerpDiscoveryResult = isCacheValid
    ? serpCache
    : await discoverSerpUrls({
        companyName: prospect.companyName,
        domain: prospect.domain,
      });

  // Persist cache in inputSnapshot for future retries
  if (!isCacheValid) {
    await db.researchRun.update({
      where: { id: run.id },
      data: {
        inputSnapshot: toJson({
          manualUrls: input.manualUrls,
          campaignId: input.campaignId,
          deepCrawl: true,
          serpCache: serpResult,
        }),
      },
    });
  }

  // Extract content via Crawl4AI for discovered URLs
  const serpUrls = [...serpResult.reviewUrls, ...serpResult.jobUrls];
  if (serpUrls.length > 0) {
    const serpEvidenceDrafts = await ingestCrawl4aiEvidenceDrafts(serpUrls);
    evidenceDrafts.push(...serpEvidenceDrafts);
  }
}
```

IMPORTANT implementation details:

- The `evidenceDrafts` variable is currently `const`. Change it from `const` to `let` so that the deepCrawl branch can push to it. OR change the `const evidenceDrafts = dedupeEvidenceDrafts(...)` pattern: collect all drafts first, THEN dedupe. The cleanest approach is to collect all evidence draft sources into a single array before deduplication:

Restructure the evidence collection to:

1. Collect `reviewEvidenceDrafts`, `websiteEvidenceDrafts`, `baseEvidenceDrafts` into a `let allDrafts` array
2. If `deepCrawl`, append SERP-discovered drafts to `allDrafts`
3. Then dedupe: `const evidenceDrafts = dedupeEvidenceDrafts(allDrafts).slice(0, 24)`

This preserves the existing behavior when deepCrawl is false (same drafts, same dedup, same cap) while cleanly adding the new source.

- `dedupeEvidenceDrafts` already handles URL+tag+snippet dedup, which will catch overlapping URLs between default research URLs and SERP-discovered URLs.
- The `.slice(0, 24)` cap remains -- even with SERP drafts, total evidence is bounded.
- When deepCrawl is false/absent, NO SerpAPI or Crawl4AI calls are made -- the function behaves identically to before.

Run `npm run check` to verify no type errors.
</action>
<verify>`npm run check` passes. The deep crawl branch is present and structurally correct. Existing tests (if any) still pass with `npx vitest run`.</verify>
<done>When deepCrawl is true, executeResearchRun discovers URLs via SerpAPI (with 24h cache), extracts content via Crawl4AI, merges into evidence drafts, deduplicates, and persists. When deepCrawl is false, function behaves identically to before.</done>
</task>

</tasks>

<verification>
- `npm run check` -- no type or lint errors
- `npx vitest run` -- all existing tests still pass
- `deepCrawl: true` in startRun input triggers SerpAPI + Crawl4AI calls
- `deepCrawl: false` (or absent) runs the original pipeline unchanged
- SerpAPI results cached in inputSnapshot with 24h TTL
- retryRun preserves deepCrawl flag from original run's inputSnapshot
- Evidence from SERP discovery is deduped with existing evidence before persistence
- Total evidence cap (24 items) still enforced
</verification>

<success_criteria>

1. Research startRun mutation accepts `deepCrawl: true` and triggers full deep evidence pipeline
2. Re-running the same prospect within 24h reuses cached SerpAPI results (no duplicate API calls)
3. SERP-discovered evidence is deduped against default research URLs
4. When deepCrawl is absent, the function is unchanged from its current behavior
5. All type checks and existing tests pass
   </success_criteria>

<output>
After completion, create `.planning/phases/08-deep-evidence-pipeline/08-03-SUMMARY.md`
</output>
