---
phase: 08-deep-evidence-pipeline
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - lib/enrichment/serp.ts
  - lib/enrichment/serp.test.ts
  - env.mjs
  - .env.example
  - package.json
autonomous: true
user_setup:
  - service: serpapi
    why: 'Google search discovery for review and job listing URLs'
    env_vars:
      - name: SERP_API_KEY
        source: 'serpapi.com Dashboard -> API Key (free tier: 250 searches/month)'

must_haves:
  truths:
    - 'discoverSerpUrls returns reviewUrls and jobUrls arrays for a given prospect'
    - 'When SERP_API_KEY is missing, discoverSerpUrls returns empty arrays without throwing'
    - 'Google Maps two-step flow fetches data_id first, then reviews using that data_id'
    - 'Google Jobs discovery returns job listing URLs from jobs_results'
    - 'Result includes discoveredAt timestamp and optional mapsDataId for cache persistence'
  artifacts:
    - path: 'lib/enrichment/serp.ts'
      provides: 'SerpAPI URL discovery client with discoverSerpUrls function'
      exports: ['discoverSerpUrls', 'SerpDiscoveryResult']
    - path: 'lib/enrichment/serp.test.ts'
      provides: 'Unit tests for SerpAPI discovery with mocked API responses'
      contains: 'discoverSerpUrls'
    - path: 'env.mjs'
      provides: 'SERP_API_KEY and CRAWL4AI_BASE_URL env var declarations'
      contains: 'SERP_API_KEY'
  key_links:
    - from: 'lib/enrichment/serp.ts'
      to: 'serpapi'
      via: 'getJson import from serpapi npm package'
      pattern: 'import.*from.*serpapi'
    - from: 'lib/enrichment/serp.ts'
      to: 'env.mjs'
      via: 'SERP_API_KEY env var access'
      pattern: "env\\.SERP_API_KEY"
---

<objective>
Create the SerpAPI URL discovery client that finds Google Maps review URLs and job listing URLs for a prospect.

Purpose: SerpAPI is the evidence discovery layer -- it finds URLs that Crawl4AI (plan 08-02) will later extract content from. Discovery and extraction are separate concerns; this plan handles discovery only.

Output: `lib/enrichment/serp.ts` exporting `discoverSerpUrls()` with TDD tests, plus env var configuration.
</objective>

<execution_context>
@/home/klarifai/.claude/get-shit-done/workflows/execute-plan.md
@/home/klarifai/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-deep-evidence-pipeline/08-RESEARCH.md

# Existing adapter patterns to follow

@lib/web-evidence-adapter.ts
@lib/review-adapters.ts

# Env var configuration

@env.mjs
@.env.example
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install serpapi package and add env vars</name>
  <files>package.json, env.mjs, .env.example</files>
  <action>
Run `npm install serpapi` to add the SerpAPI npm package (v2.2.1, TypeScript-first).

In `env.mjs`, add two new optional server env vars:

- `SERP_API_KEY: z.string().min(1).optional()` -- SerpAPI authentication key
- `CRAWL4AI_BASE_URL: z.string().url().optional()` -- Crawl4AI sidecar URL (used by plan 08-02, added here to keep env changes in one plan)

Add matching entries in the `runtimeEnv` section:

- `SERP_API_KEY: process.env.SERP_API_KEY`
- `CRAWL4AI_BASE_URL: process.env.CRAWL4AI_BASE_URL`

In `.env.example`, add a new section after the existing EXTERNAL APIS block:

```
# Deep evidence pipeline (Phase 8)
SERP_API_KEY="your-serpapi-key"
CRAWL4AI_BASE_URL="http://localhost:11235"
```

Do NOT modify any other env vars or sections.
</action>
<verify>`npm run build` completes without errors (env validation passes with optional vars). Verify `serpapi` appears in package.json dependencies.</verify>
<done>serpapi package installed, SERP_API_KEY and CRAWL4AI_BASE_URL declared as optional server env vars, .env.example updated.</done>
</task>

<task type="auto">
  <name>Task 2: Create SerpAPI discovery client with TDD</name>
  <files>lib/enrichment/serp.ts, lib/enrichment/serp.test.ts</files>
  <action>
**RED phase: Write tests first in `lib/enrichment/serp.test.ts`.**

Mock the `serpapi` package using `vi.mock('serpapi', ...)`. The mock should intercept `getJson` calls and return controlled responses.

Test cases:

1. **Returns empty arrays when SERP_API_KEY is not set** -- Mock env so SERP_API_KEY is undefined. Verify `discoverSerpUrls` returns `{ reviewUrls: [], jobUrls: [], discoveredAt: <ISO string> }`.
2. **Google Maps two-step flow** -- Mock `getJson` to return `{ place_results: { data_id: 'abc123' } }` for engine `google_maps`, then `{ reviews: [{ link: 'https://example.com/review1' }, { link: 'https://example.com/review2' }] }` for engine `google_maps_reviews`. Verify `reviewUrls` contains both links and `mapsDataId` is `'abc123'`.
3. **Google Maps with no data_id** -- Mock `getJson` to return `{ place_results: {} }` for `google_maps`. Verify `reviewUrls` is empty and no `google_maps_reviews` call is made.
4. **Google Jobs discovery** -- Mock `getJson` to return `{ jobs_results: [{ link: 'https://example.com/job1' }, { apply_options: [{ link: 'https://example.com/job2' }] }] }` for engine `google_jobs`. Verify `jobUrls` contains both URLs.
5. **Graceful error handling** -- Mock `getJson` to throw for both engines. Verify function returns empty arrays without throwing.
6. **Result cap** -- Mock responses with 10+ review URLs. Verify output is capped at 5 reviewUrls and 5 jobUrls.

**GREEN phase: Implement `lib/enrichment/serp.ts`.**

Follow the research code example closely:

```typescript
import { getJson, config as serpConfig } from 'serpapi';
```

Export `SerpDiscoveryResult` interface:

```typescript
export interface SerpDiscoveryResult {
  reviewUrls: string[];
  jobUrls: string[];
  mapsDataId?: string;
  discoveredAt: string;
}
```

Export `discoverSerpUrls` function accepting `{ companyName: string | null; domain: string }`.

Implementation details:

- If `SERP_API_KEY` is falsy (not set), return early with empty arrays and current timestamp. Do NOT throw.
- Set `serpConfig.api_key` from env before each call (lazy init pattern, consistent with getAnthropicClient).
- Use `companyName ?? domain` as the search query fallback.
- Google Maps: call `google_maps` engine with `gl: 'nl', hl: 'nl'`. Extract `data_id` from `place_results.data_id`.
- If `data_id` found, call `google_maps_reviews` engine with `data_id`, `hl: 'nl'`, `sort_by: 'newestFirst'`. Extract `link` from each review object.
- Google Jobs: call `google_jobs` engine with query `"${companyName} jobs vacatures"`, `gl: 'nl'`, `hl: 'nl'`. Extract `link` or `apply_options[0].link` from each job result.
- Wrap each engine call in try/catch -- log error with `console.error` and continue (non-blocking).
- Cap results: `reviewUrls.slice(0, 5)`, `jobUrls.slice(0, 5)`.
- Return `SerpDiscoveryResult` with `discoveredAt: new Date().toISOString()`.

Access env via lazy check: `const apiKey = process.env.SERP_API_KEY;` (not via env.mjs import -- serpapi config is set imperatively, and avoiding env.mjs import keeps the module testable without full env setup). The executor should use `process.env.SERP_API_KEY` directly with a guard clause, same pattern as the research example.

**REFACTOR phase:** Clean up types, ensure consistent error handling.

Run `npx vitest run lib/enrichment/serp.test.ts` -- all tests pass.
Run `npm run check` -- no lint/type errors.
</action>
<verify>`npx vitest run lib/enrichment/serp.test.ts` -- all 6 test cases pass. `npm run check` passes.</verify>
<done>discoverSerpUrls function exports SerpDiscoveryResult, handles missing API key gracefully, performs two-step Google Maps review discovery, discovers job listing URLs, caps results at 5 each, and handles API errors without throwing.</done>
</task>

</tasks>

<verification>
- `npx vitest run lib/enrichment/serp.test.ts` -- all tests pass
- `npm run check` -- no type or lint errors
- `serpapi` appears in package.json dependencies
- `SERP_API_KEY` declared in env.mjs server section as optional
- `CRAWL4AI_BASE_URL` declared in env.mjs server section as optional
- `.env.example` has both new env vars documented
</verification>

<success_criteria>

1. `discoverSerpUrls({ companyName: 'Acme', domain: 'acme.nl' })` returns `SerpDiscoveryResult` with review and job URL arrays
2. Missing SERP_API_KEY returns empty arrays without error
3. SerpAPI API errors are caught and logged, not thrown
4. All tests pass, no lint/type errors
   </success_criteria>

<output>
After completion, create `.planning/phases/08-deep-evidence-pipeline/08-01-SUMMARY.md`
</output>
