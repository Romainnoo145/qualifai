---
phase: 25-pipeline-hardening
plan: 04
type: execute
wave: 1
depends_on: []
files_modified:
  - services/scrapling/Dockerfile
  - services/scrapling/app.py
  - docker-compose.yml
  - lib/enrichment/scrapling.ts
  - lib/web-evidence-adapter.ts
autonomous: true
requirements: [PIPE-01, PIPE-02, PIPE-03]

must_haves:
  truths:
    - 'The Scrapling Docker service runs locally and responds to POST /fetch and POST /fetch-dynamic'
    - 'lib/web-evidence-adapter.ts uses the Scrapling StealthyFetcher instead of raw fetch() for website evidence ingestion'
    - 'Snippet length limit increased from 260 chars to 700 chars throughout web-evidence-adapter.ts'
    - 'TypeScript compiles without errors and existing evidence extraction logic is preserved'
  artifacts:
    - path: 'services/scrapling/Dockerfile'
      provides: 'Scrapling service built on ghcr.io/d4vinci/scrapling:latest with FastAPI wrapper'
      contains: 'FROM ghcr.io/d4vinci/scrapling:latest'
    - path: 'services/scrapling/app.py'
      provides: 'FastAPI app with POST /fetch (StealthyFetcher) and POST /fetch-dynamic (DynamicFetcher)'
      contains: 'StealthyFetcher'
    - path: 'docker-compose.yml'
      provides: 'scrapling service on port 3010 added'
      contains: 'qualifai-scrapling'
    - path: 'lib/enrichment/scrapling.ts'
      provides: 'TypeScript client for the Scrapling HTTP service'
      contains: 'fetchStealth'
    - path: 'lib/web-evidence-adapter.ts'
      provides: 'ingestWebsiteEvidenceDrafts now uses Scrapling StealthyFetcher with fallback to raw fetch'
      contains: 'fetchStealth'
  key_links:
    - from: 'lib/web-evidence-adapter.ts'
      to: 'lib/enrichment/scrapling.ts'
      via: 'fetchStealth(url)'
      pattern: 'fetchStealth'
    - from: 'lib/enrichment/scrapling.ts'
      to: 'http://localhost:3010'
      via: 'POST /fetch'
      pattern: 'SCRAPLING_BASE_URL'
---

<objective>
Integrate the Scrapling Python scraping library as a Docker microservice and wire it into the website evidence ingestion pipeline. Replace the current raw `fetch()` with Scrapling's `StealthyFetcher`, which bypasses bot detection and returns full page HTML. Increase snippet length limits from 260 to 700 chars to give the AI hypothesis generator substantially more content to work with.

Root cause confirmed: evidence snippets for the 5 imported prospects are mostly placeholder text ("Review source ingested but no signals extracted", "Source queued for manual validation") because the raw `fetch()` approach gets blocked or times out on bot-protected pages. Scrapling's StealthyFetcher handles these cases without a browser, keeping latency low.

Output:

- `services/scrapling/` — Dockerfile + FastAPI app wrapping Scrapling
- `docker-compose.yml` — scrapling service on port 3010
- `lib/enrichment/scrapling.ts` — TypeScript HTTP client
- `lib/web-evidence-adapter.ts` — uses Scrapling with fallback + longer snippets
  </objective>

<execution_context>
@/home/klarifai/.claude/get-shit-done/workflows/execute-plan.md
@/home/klarifai/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
Key files to read before implementing:

- `lib/web-evidence-adapter.ts` — current `ingestWebsiteEvidenceDrafts()` uses raw fetch(); `extractMetaDescription()` slices at 260; `firstReadableSnippet()` slices at 220
- `lib/enrichment/crawl4ai.ts` — existing HTTP-service pattern to replicate for scrapling.ts
- `docker-compose.yml` — current services (db on 5433, redis on 6381); add scrapling on 3010
- `.env` or `.env.example` — check if SCRAPLING_BASE_URL needs adding
  </context>

<tasks>

<task type="auto">
  <name>Task 1: Build Scrapling Docker service</name>
  <files>services/scrapling/Dockerfile, services/scrapling/app.py, docker-compose.yml</files>
  <action>
**1. Create `services/scrapling/Dockerfile`:**

```dockerfile
FROM ghcr.io/d4vinci/scrapling:latest

WORKDIR /app

RUN pip install fastapi uvicorn

COPY app.py .

CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "3010"]
```

**2. Create `services/scrapling/app.py`:**

```python
from fastapi import FastAPI
from pydantic import BaseModel
from scrapling.fetchers import StealthyFetcher, PlayWrightFetcher

app = FastAPI()


class FetchRequest(BaseModel):
    url: str


class FetchResponse(BaseModel):
    success: bool
    html: str
    status_code: int
    error: str | None = None


@app.post("/fetch", response_model=FetchResponse)
async def fetch_stealth(req: FetchRequest) -> FetchResponse:
    """
    StealthyFetcher: requests-based with fingerprint spoofing.
    Fast (~2s). Bypasses most bot detection without a browser.
    """
    try:
        fetcher = StealthyFetcher()
        page = fetcher.fetch(req.url)
        return FetchResponse(
            success=True,
            html=page.html_content or "",
            status_code=page.status or 200,
        )
    except Exception as e:
        return FetchResponse(success=False, html="", status_code=0, error=str(e))


@app.post("/fetch-dynamic", response_model=FetchResponse)
async def fetch_dynamic(req: FetchRequest) -> FetchResponse:
    """
    PlayWrightFetcher: full headless browser with stealth.
    Slower (~10s). Use for JS-heavy or heavily protected pages.
    """
    try:
        fetcher = PlayWrightFetcher()
        page = fetcher.fetch(req.url)
        return FetchResponse(
            success=True,
            html=page.html_content or "",
            status_code=page.status or 200,
        )
    except Exception as e:
        return FetchResponse(success=False, html="", status_code=0, error=str(e))


@app.get("/health")
async def health() -> dict:
    return {"status": "ok"}
```

**3. Add to `docker-compose.yml`:**

Add a `scrapling` service after `redis`:

```yaml
scrapling:
  build: ./services/scrapling
  container_name: qualifai-scrapling
  restart: unless-stopped
  ports:
    - '3010:3010'
```

**4. Build and start the service:**

```bash
cd /home/klarifai/Documents/klarifai/projects/qualifai
docker compose build scrapling
docker compose up -d scrapling
```

Wait ~30 seconds for startup, then verify:

```bash
curl -s http://localhost:3010/health
```

Expected: `{"status":"ok"}`
</action>
<verify>
<automated>curl -s http://localhost:3010/health</automated>
<manual>Service responds with {"status":"ok"}</manual>
</verify>
<done> - `services/scrapling/Dockerfile` exists and uses `ghcr.io/d4vinci/scrapling:latest` - `services/scrapling/app.py` exists with `/fetch` and `/fetch-dynamic` endpoints - `docker-compose.yml` has `scrapling` service on port 3010 - `docker compose up -d scrapling` starts successfully - Health endpoint returns ok
</done>
</task>

<task type="auto">
  <name>Task 2: Add TypeScript Scrapling client</name>
  <files>lib/enrichment/scrapling.ts</files>
  <action>
Create `lib/enrichment/scrapling.ts` following the same pattern as `lib/enrichment/crawl4ai.ts`.

The client should:

1. Read `SCRAPLING_BASE_URL` from env (default: `http://localhost:3010`)
2. Export `fetchStealth(url: string): Promise<{ html: string; ok: boolean }>` — calls POST /fetch
3. Export `fetchDynamic(url: string): Promise<{ html: string; ok: boolean }>` — calls POST /fetch-dynamic
4. Both functions: 30-second timeout via AbortController, return `{ html: '', ok: false }` on any error
5. Log errors to console but never throw — callers handle fallback

```typescript
const SCRAPLING_BASE_URL =
  process.env.SCRAPLING_BASE_URL ?? 'http://localhost:3010';

interface ScraplingResponse {
  success: boolean;
  html: string;
  status_code: number;
  error?: string | null;
}

async function scraplingFetch(
  endpoint: '/fetch' | '/fetch-dynamic',
  url: string,
): Promise<{ html: string; ok: boolean }> {
  const controller = new AbortController();
  const timeoutId = setTimeout(() => controller.abort(), 30000);
  try {
    const response = await fetch(`${SCRAPLING_BASE_URL}${endpoint}`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ url }),
      signal: controller.signal,
    });
    clearTimeout(timeoutId);
    if (!response.ok) return { html: '', ok: false };
    const data = (await response.json()) as ScraplingResponse;
    return { html: data.html ?? '', ok: data.success && data.html.length > 0 };
  } catch (err) {
    clearTimeout(timeoutId);
    console.error('scrapling fetch failed for', url, err);
    return { html: '', ok: false };
  }
}

export async function fetchStealth(
  url: string,
): Promise<{ html: string; ok: boolean }> {
  return scraplingFetch('/fetch', url);
}

export async function fetchDynamic(
  url: string,
): Promise<{ html: string; ok: boolean }> {
  return scraplingFetch('/fetch-dynamic', url);
}
```

After writing the file, verify TypeScript:

```bash
cd /home/klarifai/Documents/klarifai/projects/qualifai && npx tsc --noEmit 2>&1 | head -20
```

  </action>
  <verify>
    <automated>cd /home/klarifai/Documents/klarifai/projects/qualifai && npx tsc --noEmit 2>&1 | head -20</automated>
    <manual>fetchStealth and fetchDynamic are exported, file follows crawl4ai.ts pattern.</manual>
  </verify>
  <done>
    - `lib/enrichment/scrapling.ts` exists
    - `fetchStealth` and `fetchDynamic` exported
    - TypeScript compiles without errors
  </done>
</task>

<task type="auto">
  <name>Task 3: Wire Scrapling into web-evidence-adapter and increase snippet limits</name>
  <files>lib/web-evidence-adapter.ts</files>
  <action>
Two changes in `lib/web-evidence-adapter.ts`:

**Change 1: Increase snippet length limits**

Update `extractMetaDescription` — change the slice from 260 to 700:

```typescript
return value.length >= 30 ? value.slice(0, 700) : null;
```

Update `firstReadableSnippet` — change filter max from 260 to 700, and fallback slice from 220 to 600:

```typescript
.filter((line) => line.length >= 35 && line.length <= 700);
return sentences[0] ?? normalizeWhitespace(text).slice(0, 600);
```

**Change 2: Replace raw fetch() with Scrapling in `ingestWebsiteEvidenceDrafts`**

Add import at top of file:

```typescript
import { fetchStealth } from '@/lib/enrichment/scrapling';
```

In `ingestWebsiteEvidenceDrafts`, replace the `fetch()` block with Scrapling + fallback:

Current pattern (lines ~266-308):

```typescript
const controller = new AbortController();
const timeout = setTimeout(() => controller.abort(), 9000);
const response = await fetch(sourceUrl, { ... });
clearTimeout(timeout);

if (!response.ok) { ... }
const html = await response.text();
```

Replace with:

```typescript
// Try Scrapling StealthyFetcher first (bypasses bot detection)
let html: string | null = null;

const scrapling = await fetchStealth(sourceUrl);
if (scrapling.ok && scrapling.html.length > 200) {
  html = scrapling.html;
} else {
  // Fallback: raw fetch (for local/intranet pages or when Scrapling service is down)
  try {
    const controller = new AbortController();
    const timeout = setTimeout(() => controller.abort(), 9000);
    const response = await fetch(sourceUrl, {
      method: 'GET',
      headers: {
        'user-agent':
          'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Qualifai/1.0',
        accept:
          'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
      },
      signal: controller.signal,
    });
    clearTimeout(timeout);
    if (response.ok) {
      html = await response.text();
    } else if (response.status === 404 || response.status === 410) {
      continue;
    } else {
      drafts.push(fallbackDraft(sourceUrl, sourceType));
      continue;
    }
  } catch {
    drafts.push(fallbackDraft(sourceUrl, sourceType));
    continue;
  }
}

if (!html) {
  drafts.push(fallbackDraft(sourceUrl, sourceType));
  continue;
}
```

The rest of the logic (soft404 check, `extractWebsiteEvidenceFromHtml`) stays identical — it still receives `html: string`.

After changes, run full TypeScript check:

```bash
cd /home/klarifai/Documents/klarifai/projects/qualifai && npx tsc --noEmit 2>&1 | head -30
```

Fix any type errors before committing.
</action>
<verify>
<automated>cd /home/klarifai/Documents/klarifai/projects/qualifai && npx tsc --noEmit 2>&1 | head -30</automated>
<manual>Confirm fetchStealth import present, snippet slice constants updated to 700/600, fallback fetch preserved.</manual>
</verify>
<done> - `lib/web-evidence-adapter.ts` imports fetchStealth from scrapling - Snippet length limits: meta description 700, firstReadableSnippet 700/600 - Scrapling used as primary, raw fetch as fallback - TypeScript compiles without errors
</done>
</task>

<task type="auto">
  <name>Task 4: Test Scrapling on one prospect and confirm improved snippet quality</name>
  <files>None (verification only)</files>
  <action>
Test the Scrapling integration by checking if it returns real HTML for one of the previously blocked domains.

**Quick smoke test via the service:**

```bash
curl -s -X POST http://localhost:3010/fetch \
  -H "Content-Type: application/json" \
  -d '{"url": "https://motiondesignawards.com"}' \
  | python3 -c "import sys,json; d=json.load(sys.stdin); print('ok:', d['success'], 'html_length:', len(d.get('html','')))"
```

If the service returns `ok: True` and `html_length > 1000`, the integration is working.

Also check us3consulting.co.uk:

```bash
curl -s -X POST http://localhost:3010/fetch \
  -H "Content-Type: application/json" \
  -d '{"url": "https://www.us3consulting.co.uk"}' \
  | python3 -c "import sys,json; d=json.load(sys.stdin); print('ok:', d['success'], 'html_length:', len(d.get('html','')))"
```

Document the results: html_length before (known from previous run) vs html_length now.

If the service is not yet up or returns errors, note this in the SUMMARY — the architecture is correct but the Docker build may need manual start on next session.
</action>
<verify>
<automated>curl -s http://localhost:3010/health | python3 -c "import sys,json; d=json.load(sys.stdin); print('health:', d)"</automated>
<manual>At least one of the two test URLs returns html_length > 1000.</manual>
</verify>
<done> - Scrapling service health check passes - At least one previously blocked domain returns HTML > 1000 chars via /fetch - Results documented in SUMMARY.md
</done>
</task>

</tasks>

<verification>
Overall checks after all tasks complete:

1. `docker compose ps` shows `qualifai-scrapling` running
2. `curl http://localhost:3010/health` returns `{"status":"ok"}`
3. `npx tsc --noEmit` passes with 0 errors
4. `lib/web-evidence-adapter.ts` imports from `@/lib/enrichment/scrapling`
5. Snippet length constants updated (700 / 600 chars)
6. `services/scrapling/Dockerfile` uses `ghcr.io/d4vinci/scrapling:latest`
   </verification>

<success_criteria>
Plan 25-04 is complete when:

- The Scrapling Docker service is running on port 3010 and returns real HTML for test URLs
- The website evidence pipeline uses Scrapling as its primary fetcher with raw fetch() as fallback
- Snippet limits increased to 700/600 chars
- TypeScript compiles cleanly
  </success_criteria>

<output>
After completion, create `.planning/phases/25-pipeline-hardening/25-04-SUMMARY.md` following the summary template.

Update ROADMAP.md to add `25-04-PLAN.md` to the phase plan list and mark it complete.
</output>
