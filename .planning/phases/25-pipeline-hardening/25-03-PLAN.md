---
phase: 25-pipeline-hardening
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - lib/workflow-engine.ts
  - lib/research-executor.ts
  - .planning/phases/25-pipeline-hardening/25-03-NOTES.md
autonomous: false
requirements: [PIPE-01, PIPE-02, PIPE-03]

must_haves:
  truths:
    - 'Every imported real prospect has at least one hypothesis whose title and problem statement are specific to a Dutch marketing agency workflow pain (not generic construction/field-service templates)'
    - 'The AI hypothesis generator reads actual evidence snippets from the research run and derives industry-aware pain hypotheses — not fixed templates'
    - 'Re-running research on all 5 imported prospects produces hypotheses that pass at least amber quality gate score'
    - '25-03-NOTES.md documents which hypotheses passed, which needed fixing, what changed in the generator, and any prospects still flagged as low-confidence'
  artifacts:
    - path: 'lib/workflow-engine.ts'
      provides: 'generateHypothesisDrafts now accepts evidence with snippets and calls Gemini for AI-driven synthesis; hardcoded construction templates replaced'
      contains: 'generateHypothesisDraftsAI'
    - path: 'lib/research-executor.ts'
      provides: 'executeResearchRun passes evidence content (snippet field) to the new AI hypothesis generator'
    - path: '.planning/phases/25-pipeline-hardening/25-03-NOTES.md'
      provides: 'Manual review findings for all 5 imported prospects'
  key_links:
    - from: 'lib/research-executor.ts'
      to: 'lib/workflow-engine.ts'
      via: 'generateHypothesisDraftsAI(evidenceRecords)'
      pattern: 'generateHypothesisDraftsAI'
    - from: 'lib/workflow-engine.ts'
      to: 'gemini-2.0-flash'
      via: 'getGenAI().getGenerativeModel'
      pattern: 'getGenerativeModel'
---

<objective>
Replace the hardcoded construction-industry hypothesis templates with an AI-driven generator that reads real evidence snippets and derives marketing-agency-specific workflow pain hypotheses. Re-run research on all imported real prospects and manually review the results in the admin UI. Document the outcome in 25-03-NOTES.md.

Purpose: The research pipeline currently produces 3 fixed hypotheses regardless of the prospect's industry ("planning bottleneck", "office-to-field handoff", "quote-to-invoice") — these are construction/field-service templates that are wrong for Dutch marketing agencies. Outreach cannot be personalized if the hypotheses don't reflect real pain. This must be fixed before Phase 26 calibration or Phase 27 outreach.

Output:

- Updated `lib/workflow-engine.ts` with AI-powered `generateHypothesisDraftsAI()`
- Updated `lib/research-executor.ts` wired to the new generator
- `25-03-NOTES.md` documenting hypothesis quality per prospect after re-run
  </objective>

<execution_context>
@/home/klarifai/.claude/get-shit-done/workflows/execute-plan.md
@/home/klarifai/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/25-pipeline-hardening/25-CONTEXT.md
@.planning/phases/25-pipeline-hardening/25-01-SUMMARY.md
@.planning/phases/25-pipeline-hardening/25-02-SUMMARY.md

Key files to read before implementing:

- lib/workflow-engine.ts — contains `generateHypothesisDrafts()` (the hardcoded template function to replace) and `HypothesisDraft` interface, `getGenAI()` helper
- lib/research-executor.ts — calls `generateHypothesisDrafts(evidenceRecords)` at line ~576, evidenceRecords only carry {id, sourceType, workflowTag, confidenceScore} — NO snippets
- lib/ai/prompts.ts — existing SYSTEM_PROMPT and prompt-building patterns to follow
  </context>

<tasks>

<task type="auto">
  <name>Task 1: Replace hardcoded hypothesis generator with AI-driven evidence synthesis</name>
  <files>lib/workflow-engine.ts</files>
  <action>
In `lib/workflow-engine.ts`:

1. Keep the existing `HypothesisDraft` interface and `generateHypothesisDrafts()` as a fallback (rename to `generateFallbackHypothesisDrafts` — keep internal, not exported).

2. Add a new exported async function `generateHypothesisDraftsAI()` with this signature:

```typescript
export async function generateHypothesisDraftsAI(
  evidence: Array<{
    id: string;
    sourceType: EvidenceSourceType;
    workflowTag: string;
    confidenceScore: number;
    snippet: string;
    sourceUrl: string;
    title: string | null;
  }>,
  prospectContext: {
    companyName: string | null;
    industry: string | null;
    specialties: string[];
    description: string | null;
  },
): Promise<HypothesisDraft[]>;
```

3. The function should:
   a. Filter evidence to those with `confidenceScore >= 0.6` and non-empty `snippet`.
   b. Take up to 12 of the highest-confidence evidence items (sort descending).
   c. Build a prompt string for Gemini that:
   - States the company context (name, industry, specialties, description)
   - Lists each evidence item as: `[sourceType] sourceUrl: snippet (first 200 chars)`
   - Asks for 3 distinct workflow pain hypotheses specific to this company's industry
   - Emphasizes: Dutch marketing bureau context, AI/workflow automation pain points, must be specific and personalizable (not generic "they want to grow")
   - Instructs Claude to output ONLY a JSON array with this schema per item:
     ```json
     [
       {
         "title": "...",
         "problemStatement": "...",
         "assumptions": ["..."],
         "validationQuestions": ["...", "..."],
         "workflowTag": "...",
         "confidenceScore": 0.0,
         "evidenceRefs": ["<sourceUrl>", "..."]
       }
     ]
     ```
   - `confidenceScore` should reflect how specifically the evidence supports the pain (0.6–0.9 range)
   - `workflowTag` must be one of: `content-production`, `client-reporting`, `project-management`, `lead-intake`, `creative-briefing`, `billing`, `handoff`

   d. Call Gemini (`gemini-2.0-flash`, same model as other callers in this file) with the prompt.
   e. Parse the JSON response — extract the array using `.match(/\[[\s\S]*\]/)`
   f. Map each parsed item to `HypothesisDraft` using the existing metric defaults (same numbers as the old templates: `hoursSavedWeekLow: 4`, `hoursSavedWeekMid: 8`, `hoursSavedWeekHigh: 14`, `handoffSpeedGainPct: 28`, `errorReductionPct: 20`, `revenueLeakageRecoveredLow: 400`, `revenueLeakageRecoveredMid: 900`, `revenueLeakageRecoveredHigh: 2000`)
   g. Map `evidenceRefs` from the parsed sourceUrls back to evidence IDs: for each ref URL, find the matching evidence item by sourceUrl, use its `id`. Fall back to `defaultEvidenceRefs(evidence)` if no match.
   h. Return the array. If parsing fails or array is empty, catch the error and call `generateFallbackHypothesisDrafts(evidence)` (the renamed existing function) as fallback.

4. Important: do NOT use `SYSTEM_PROMPT` from `lib/ai/prompts.ts` — build a standalone prompt string directly in this function for clarity.

5. Type all locals — no `any`.

The prompt should explicitly say (in English):

```
You are analyzing evidence from a Dutch marketing bureau's public web presence to identify specific workflow automation pain points that Klarifai (an AI implementation consultancy) could solve.

IMPORTANT: These are marketing agencies (bureaus), not construction companies. Focus on marketing/creative workflow pain: content production bottlenecks, client reporting overhead, briefing-to-delivery friction, project management chaos, manual campaign reporting, etc.

Only output JSON. No markdown fences. No explanation.
```

  </action>
  <verify>
    <automated>cd /home/klarifai/Documents/klarifai/projects/qualifai && npx tsc --noEmit 2>&1 | head -30</automated>
    <manual>Confirm `generateHypothesisDraftsAI` is exported and `generateFallbackHypothesisDrafts` is defined internally. Confirm the old `generateHypothesisDrafts` export is preserved as a re-export alias or the executor is updated in Task 2.</manual>
  </verify>
  <done>
    - `generateHypothesisDraftsAI` is exported from `lib/workflow-engine.ts`
    - The function calls Gemini and falls back to templates on failure
    - TypeScript compiles without errors on this file
    - The old `generateHypothesisDrafts` export is either kept as alias or removed (executor updated in Task 2)
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire AI hypothesis generator into research executor and re-run prospects</name>
  <files>lib/research-executor.ts</files>
  <action>
In `lib/research-executor.ts`:

1. Add `generateHypothesisDraftsAI` to the import from `@/lib/workflow-engine`. Remove the old `generateHypothesisDrafts` import (or keep if needed as fallback, but the AI version should be primary).

2. In `executeResearchRun`, update the evidence selection step before calling the hypothesis generator. The current call at line ~576 passes `evidenceRecords.map(item => ({id, sourceType, workflowTag, confidenceScore}))` — this drops the `snippet` and `sourceUrl` fields needed by the AI generator.

   Change to pass the full evidence record shape that `generateHypothesisDraftsAI` expects:

   ```typescript
   const hypotheses = await generateHypothesisDraftsAI(
     evidenceRecords.map((item) => ({
       id: item.id,
       sourceType: item.sourceType,
       workflowTag: item.workflowTag,
       confidenceScore: item.confidenceScore,
       snippet: item.snippet,
       sourceUrl: item.sourceUrl,
       title: item.title,
     })),
     {
       companyName: prospect.companyName,
       industry: prospect.industry,
       specialties: prospect.specialties,
       description: prospect.description,
     },
   );
   ```

3. After the change, run TypeScript check and fix any type errors.

4. After the code change compiles cleanly, re-run research on all 5 imported prospects via the admin UI or CLI script. Use the `executeResearchRun` function. The goal is to produce fresh hypotheses using the AI generator against the previously collected evidence.

   Re-run command (execute via node script using dotenv):

   ```bash
   cd /home/klarifai/Documents/klarifai/projects/qualifai
   node -e "
   require('dotenv').config();
   const { PrismaPg } = require('@prisma/adapter-pg');
   const { PrismaClient } = require('@prisma/client');
   const { Pool } = require('pg');
   const pool = new Pool({ connectionString: process.env.DATABASE_URL });
   const prisma = new PrismaClient({ adapter: new PrismaPg(pool) });
   const { executeResearchRun } = require('./lib/research-executor');

   async function main() {
     const prospects = await prisma.prospect.findMany({ select: { id: true, domain: true } });
     console.log('Prospects:', prospects.map(p => p.domain).join(', '));
     for (const p of prospects) {
       const existing = await prisma.researchRun.findFirst({ where: { prospectId: p.id }, orderBy: { createdAt: 'desc' } });
       const result = await executeResearchRun(prisma, {
         prospectId: p.id,
         manualUrls: [],
         existingRunId: existing?.id,
         deepCrawl: false,
       });
       console.log(p.domain, '→ hypotheses:', result.counts.hypotheses, 'evidence:', result.counts.evidence);
     }
     await prisma.\$disconnect();
   }
   main().catch(console.error);
   "
   ```

   Note: If the script needs TypeScript compilation first, compile via `npx tsc` or use `ts-node` if available. If the Node.js direct approach fails due to TypeScript, use the app's existing tRPC research endpoint via the running dev server at port 9200 instead.

5. After re-runs complete, note the output (hypothesis count per prospect) for Task 3.
   </action>
   <verify>
   <automated>cd /home/klarifai/Documents/klarifai/projects/qualifai && npx tsc --noEmit 2>&1 | head -30</automated>
   <manual>Check that re-run completes without errors. The output should show hypothesis count per prospect.</manual>
   </verify>
   <done> - `research-executor.ts` imports and calls `generateHypothesisDraftsAI` with full evidence shape including snippet and sourceUrl - TypeScript compiles without errors - Re-run on all imported prospects completes (or at least 3 prospects if some fail) - Each re-run produces at least 1 hypothesis (or falls back gracefully to templates)
   </done>
   </task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 3: Manual review of hypothesis quality in admin UI and document findings</name>
  <files>.planning/phases/25-pipeline-hardening/25-03-NOTES.md</files>
  <action>Human reviews hypothesis quality in admin UI and documents findings in 25-03-NOTES.md.</action>
  <what-built>
    - AI-driven hypothesis generator that reads real evidence snippets and produces marketing-agency-specific pain hypotheses
    - Research executor wired to the new generator
    - Re-run executed on all imported real prospects
  </what-built>
  <how-to-verify>
    1. Open the admin UI at http://localhost:9200/admin/prospects
    2. For each imported prospect (hydrogen-central.com, deondernemer.nl, motiondesignawards.com, us3consulting.co.uk, cybersecuritydistrict.com):
       a. Open the prospect detail page
       b. Click the "Analysis" tab
       c. Read each hypothesis title and problem statement
       d. Judge: is this specific to a marketing/creative/digital agency workflow pain? Could it anchor a credible personalized outreach email?
       e. Mark each hypothesis: PASS (specific, plausible) or FAIL (generic/wrong industry/hallucinated)
    3. Check that the quality gate chip shows amber or green for at least 3 prospects (not red)
    4. Note any prospects where ALL hypotheses fail — these need flagging in NOTES.md

    After review, create `.planning/phases/25-pipeline-hardening/25-03-NOTES.md` documenting:
    - Per-prospect: which hypotheses passed, which failed, why
    - What changed in the generator vs old templates
    - Any prospects flagged as low-confidence / thin evidence (input for Phase 28)
    - Overall verdict: is hypothesis quality production-ready for Dutch marketing agency outreach?

    Type "approved" if at least 4/5 prospects have at least 1 passing hypothesis. Type "needs-fix: [describe issues]" if the generator still produces bad output and prompts need further iteration.

  </how-to-verify>
  <verify>Human reviews hypotheses in admin UI at http://localhost:9200/admin/prospects and confirms at least 4/5 prospects have 1+ passing hypothesis.</verify>
  <done>25-03-NOTES.md exists and documents per-prospect hypothesis quality findings. All 5 imported real prospects have at least 1 passing hypothesis OR are explicitly flagged as low-confidence requiring more evidence sources (Phase 28 input). TypeScript and ESLint pass.</done>
  <resume-signal>Type "approved" or "needs-fix: [describe what's wrong]"</resume-signal>
</task>

</tasks>

<verification>
Overall phase checks after all tasks complete:

1. `npx tsc --noEmit` passes with 0 errors
2. `generateHypothesisDraftsAI` exported from `lib/workflow-engine.ts`
3. `lib/research-executor.ts` calls `generateHypothesisDraftsAI` with snippet + sourceUrl in the evidence map
4. At least 3 imported prospects show marketing-agency-specific hypotheses in admin UI Analysis tab
5. At least 3 imported prospects show amber or green quality gate chip
6. `.planning/phases/25-pipeline-hardening/25-03-NOTES.md` exists and documents per-prospect findings
   </verification>

<success_criteria>
Phase 25-03 is complete when:

- ALL imported real prospects have at least 1 hypothesis that a human reviewer judges as specific and plausible for a Dutch marketing agency (or are explicitly flagged as low-confidence in NOTES.md)
- The AI generator is wired into the research pipeline (not the old hardcoded templates)
- 25-03-NOTES.md documents the validation findings
- TypeScript and ESLint pass
  </success_criteria>

<output>
After completion, create `.planning/phases/25-pipeline-hardening/25-03-SUMMARY.md` following the summary template.

Also update ROADMAP.md to mark `25-03-PLAN.md` as complete (change `- [ ]` to `- [x]`).
</output>
