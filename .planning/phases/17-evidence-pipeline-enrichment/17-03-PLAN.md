---
phase: 17-evidence-pipeline-enrichment
plan: '03'
type: execute
wave: 2
depends_on: ['17-01', '17-02']
files_modified:
  - lib/research-executor.ts
autonomous: true

must_haves:
  truths:
    - 'Research run with sitemap.xml uses discovered URLs instead of guessed /careers /jobs /docs /help paths'
    - 'Research run falls back to defaultResearchUrls when sitemap returns empty'
    - 'Research run with deepCrawl=true runs Google search mention queries via SerpAPI'
    - 'Google search mentions become evidence drafts stored in the database'
    - 'KvK enrichment runs for prospects with a companyName and creates REGISTRY evidence'
    - 'Apollo-derived LinkedIn profile data becomes an evidence draft when description/specialties exist'
    - 'LinkedIn Crawl4AI extraction is attempted when linkedinUrl exists, with authwall detection'
    - 'Sitemap results are cached in inputSnapshot with 24h TTL'
    - 'All new evidence sources merge into allDrafts before single dedup+cap pass'
  artifacts:
    - path: 'lib/research-executor.ts'
      provides: 'Integrated evidence pipeline with sitemap, Google search, KvK, and LinkedIn sources'
      exports: ['executeResearchRun']
  key_links:
    - from: 'lib/research-executor.ts'
      to: 'lib/enrichment/sitemap.ts'
      via: 'import discoverSitemapUrls'
      pattern: 'import.*discoverSitemapUrls.*sitemap'
    - from: 'lib/research-executor.ts'
      to: 'lib/enrichment/serp.ts'
      via: 'import discoverGoogleSearchMentions'
      pattern: 'import.*discoverGoogleSearchMentions.*serp'
    - from: 'lib/research-executor.ts'
      to: 'lib/enrichment/kvk.ts'
      via: 'import fetchKvkData and kvkDataToEvidenceDraft'
      pattern: 'import.*fetchKvkData.*kvk'
    - from: 'lib/research-executor.ts'
      to: 'lib/enrichment/crawl4ai.ts'
      via: 'import extractMarkdown for LinkedIn'
      pattern: 'import.*extractMarkdown.*crawl4ai'
---

<objective>
Wire all four new evidence sources (sitemap, Google search, KvK, LinkedIn) into the research executor pipeline.

Purpose: This is the integration plan that makes EVID-06 through EVID-09 live. The individual modules from Plans 01 and 02 are connected to executeResearchRun so that running research on a prospect actually collects evidence from these sources. Without this plan, the modules exist but are never called.

Output: `lib/research-executor.ts` with sitemap-first URL discovery, Google search mentions, KvK registry enrichment, and LinkedIn evidence (both Apollo-derived and Crawl4AI best-effort).
</objective>

<execution_context>
@/home/klarifai/.claude/get-shit-done/workflows/execute-plan.md
@/home/klarifai/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/17-evidence-pipeline-enrichment/17-RESEARCH.md
@.planning/phases/17-evidence-pipeline-enrichment/17-01-SUMMARY.md
@.planning/phases/17-evidence-pipeline-enrichment/17-02-SUMMARY.md
@lib/research-executor.ts
@lib/enrichment/sitemap.ts
@lib/enrichment/serp.ts
@lib/enrichment/kvk.ts
@lib/enrichment/crawl4ai.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add sitemap-first URL discovery and sitemap caching</name>
  <files>lib/research-executor.ts</files>
  <action>
Modify `executeResearchRun` in `lib/research-executor.ts` to use sitemap discovery (EVID-06) as the primary URL source, with guessed paths as fallback.

1. Add imports at top:

   ```typescript
   import {
     discoverSitemapUrls,
     type SitemapCache,
   } from '@/lib/enrichment/sitemap';
   ```

2. Add `extractSitemapCache` helper function (same pattern as existing `extractSerpCache`):

   ```typescript
   function extractSitemapCache(snapshot: unknown): SitemapCache | null {
     const payload = snapshot as { sitemapCache?: SitemapCache } | null;
     if (!payload?.sitemapCache?.discoveredAt) return null;
     return payload.sitemapCache;
   }
   ```

3. Add `linkedinUrl` to the prospect select query (needed for EVID-08 in Task 2):

   ```typescript
   select: {
     id: true,
     domain: true,
     companyName: true,
     industry: true,
     employeeRange: true,
     description: true,
     technologies: true,
     specialties: true,
     linkedinUrl: true,  // NEW for EVID-08
   },
   ```

4. Replace the existing URL construction block (lines ~132-142):

   ```typescript
   // OLD:
   const researchUrls = uniqueUrls([
     ...defaultResearchUrls(prospect.domain),
     ...nonReviewManualUrls,
   ]);
   ```

   With sitemap-first approach:

   ```typescript
   // Sitemap discovery — always runs (zero API cost)
   // Check cache first (24h TTL)
   const existingSnapshot = input.existingRunId
     ? (
         await db.researchRun.findUnique({
           where: { id: input.existingRunId },
           select: { inputSnapshot: true },
         })
       )?.inputSnapshot
     : null;
   const sitemapCache = extractSitemapCache(existingSnapshot);
   const isSitemapCacheValid =
     sitemapCache &&
     Date.now() - new Date(sitemapCache.discoveredAt).getTime() <
       24 * 60 * 60 * 1000;

   const sitemapUrls = isSitemapCacheValid
     ? sitemapCache.urls
     : await discoverSitemapUrls(prospect.domain);

   // Use sitemap URLs if available, fall back to guessed paths
   const researchUrls = uniqueUrls(
     sitemapUrls.length > 0
       ? [...sitemapUrls, ...nonReviewManualUrls]
       : [...defaultResearchUrls(prospect.domain), ...nonReviewManualUrls],
   );
   ```

5. Update the `inputSnapshot` writes to include `sitemapCache`. There are two places where `inputSnapshot` is written (create and update). Both need to include `sitemapCache` when sitemap was freshly discovered:

   ```typescript
   const freshSitemapCache: SitemapCache | undefined =
     !isSitemapCacheValid && sitemapUrls.length > 0
       ? { discoveredAt: new Date().toISOString(), urls: sitemapUrls }
       : undefined;
   ```

   Then in both the create and update `inputSnapshot` objects, spread `...(freshSitemapCache ? { sitemapCache: freshSitemapCache } : {})`.

   IMPORTANT: The `existingSnapshot` read for sitemap cache happens BEFORE the run create/update. For new runs (no `existingRunId`), there's no prior snapshot, so cache is always null — that's correct. For re-runs, the prior snapshot from `existingRunId` is used.

   NOTE: The existing `serpCache` write in the deepCrawl block also needs to include the sitemapCache. When the deepCrawl block writes its updated inputSnapshot, include both `serpCache` and `sitemapCache`:

   ```typescript
   inputSnapshot: toJson({
     manualUrls: input.manualUrls,
     campaignId: input.campaignId,
     deepCrawl: true,
     serpCache: serpResult,
     ...(freshSitemapCache ? { sitemapCache: freshSitemapCache } : {}),
   }),
   ```

     </action>
     <verify>
   `npx tsc --noEmit` passes.
   `npm run check` passes.
     </verify>
     <done>
   Sitemap discovery runs on every research execution. URLs from sitemap.xml replace guessed paths. Sitemap results are cached in inputSnapshot with 24h TTL. Fallback to defaultResearchUrls when no sitemap found.
     </done>
   </task>

<task type="auto">
  <name>Task 2: Wire Google search, KvK, and LinkedIn evidence into pipeline</name>
  <files>lib/research-executor.ts</files>
  <action>
Continue modifying `executeResearchRun` to integrate the remaining three evidence sources.

1. Add imports at top (in addition to Task 1 imports):

   ```typescript
   import {
     discoverGoogleSearchMentions,
     type GoogleSearchMention,
   } from '@/lib/enrichment/serp';
   import { fetchKvkData, kvkDataToEvidenceDraft } from '@/lib/enrichment/kvk';
   import { extractMarkdown } from '@/lib/enrichment/crawl4ai';
   ```

   Note: `discoverSerpUrls` and `SerpDiscoveryResult` are already imported. Add `discoverGoogleSearchMentions` and `GoogleSearchMention` to the existing import.
   Note: `ingestCrawl4aiEvidenceDrafts` is already imported. Add `extractMarkdown` to the existing import.

2. **EVID-07 (Google search mentions):** Inside the existing `if (input.deepCrawl)` block, AFTER the existing SERP discovery + Crawl4AI extraction, add:

   ```typescript
   // Google search mentions (EVID-07) — 3 queries, gated behind deepCrawl
   try {
     const googleMentions = await discoverGoogleSearchMentions({
       companyName: prospect.companyName,
       domain: prospect.domain,
     });
     for (const mention of googleMentions) {
       allDrafts.push({
         sourceType: 'WEBSITE',
         sourceUrl: mention.url,
         title: mention.title,
         snippet: mention.snippet.slice(0, 240),
         workflowTag: 'workflow-context',
         confidenceScore: 0.71,
         metadata: { adapter: 'serp-google-search', source: 'google-mention' },
       });
     }
   } catch (err) {
     console.error('[Google Search] mention discovery failed:', err);
   }
   ```

3. **EVID-09 (KvK registry):** AFTER the deepCrawl block (outside it — KvK runs always, not gated behind deepCrawl), add:

   ```typescript
   // KvK registry enrichment (EVID-09) — runs for all Dutch prospects
   if (prospect.companyName) {
     try {
       const kvkData = await fetchKvkData(prospect.companyName);
       if (kvkData) {
         allDrafts.push(kvkDataToEvidenceDraft(kvkData));
       }
     } catch (err) {
       console.error('[KvK] enrichment failed, continuing without:', err);
     }
   }
   ```

4. **EVID-08 (LinkedIn — Apollo-derived):** AFTER the KvK block, add the reliable Apollo-derived approach. This always runs when Apollo data exists (no network call):

   ```typescript
   // LinkedIn profile evidence from Apollo-derived data (EVID-08 — reliable)
   const linkedinSnippet = [
     prospect.description,
     prospect.specialties.length > 0
       ? `Specialiteiten: ${prospect.specialties.join(', ')}`
       : null,
     prospect.industry ? `Sector: ${prospect.industry}` : null,
   ]
     .filter(Boolean)
     .join(' | ');

   if (linkedinSnippet.length > 30) {
     allDrafts.push({
       sourceType: 'WEBSITE',
       sourceUrl:
         prospect.linkedinUrl ??
         `https://www.linkedin.com/company/${prospect.domain.split('.')[0]}`,
       title: `${prospect.companyName ?? prospect.domain} - Bedrijfsprofiel`,
       snippet: linkedinSnippet.slice(0, 240),
       workflowTag: 'workflow-context',
       confidenceScore: 0.74,
       metadata: { adapter: 'apollo-derived', source: 'linkedin-profile' },
     });
   }
   ```

5. **EVID-08 (LinkedIn — Crawl4AI best-effort):** AFTER the Apollo-derived block, add the optional browser extraction attempt. Only runs when linkedinUrl exists AND deepCrawl is true (expensive operation, gated):

   ```typescript
   // LinkedIn browser extraction attempt (EVID-08 — best-effort, often blocked)
   if (input.deepCrawl && prospect.linkedinUrl) {
     try {
       const { markdown, title } = await extractMarkdown(prospect.linkedinUrl);

       // Detect LinkedIn authwall redirect
       const isAuthwall =
         !markdown ||
         markdown.length < 200 ||
         [
           'authwall',
           'log in to linkedin',
           'join linkedin',
           'sign in',
           'leden login',
         ].some((phrase) => markdown.toLowerCase().includes(phrase));

       if (!isAuthwall) {
         allDrafts.push({
           sourceType: 'WEBSITE',
           sourceUrl: prospect.linkedinUrl,
           title:
             title || `${prospect.companyName ?? prospect.domain} - LinkedIn`,
           snippet: markdown.slice(0, 240).replace(/\n+/g, ' ').trim(),
           workflowTag: 'workflow-context',
           confidenceScore: 0.72,
           metadata: { adapter: 'crawl4ai', source: 'linkedin' },
         });
       }
       // If authwall: skip silently — don't create fallback for LinkedIn
     } catch {
       // LinkedIn extraction failure is expected — skip silently
     }
   }
   ```

6. **Ordering:** The final order of evidence collection in executeResearchRun should be:
   1. Review URL evidence (existing)
   2. Sitemap-first website evidence (Task 1)
   3. Base evidence drafts (existing)
   4. DeepCrawl block: SERP discovery + Crawl4AI + Google search mentions (existing + EVID-07)
   5. KvK enrichment (EVID-09, outside deepCrawl)
   6. Apollo-derived LinkedIn (EVID-08, outside deepCrawl)
   7. Crawl4AI LinkedIn attempt (EVID-08, inside deepCrawl check)
   8. Dedup + cap (existing)

7. **Evidence cap:** The current `.slice(0, 24)` cap should be increased to `.slice(0, 36)` to accommodate the new sources. 4 new source types adding potentially 15+ drafts means 24 is too restrictive. 36 keeps the total manageable while giving all sources room.
   </action>
   <verify>
   `npx tsc --noEmit` passes.
   `npm run check` passes.
   All existing tests pass: `npx vitest run`.
   </verify>
   <done>
   executeResearchRun integrates all four new evidence sources. Google search is gated behind deepCrawl. KvK runs for all prospects with companyName. Apollo-derived LinkedIn runs always when data exists. Crawl4AI LinkedIn is best-effort behind deepCrawl. Evidence cap increased to 36.
   </done>
   </task>

</tasks>

<verification>
1. `npm run check` passes (type-check + lint + format)
2. `npx vitest run` — all existing tests pass
3. `lib/research-executor.ts` imports from all four enrichment modules: sitemap, serp (GoogleSearchMentions), kvk, crawl4ai (extractMarkdown)
4. `executeResearchRun` function contains:
   - Sitemap discovery before URL construction
   - Google search mentions inside deepCrawl block
   - KvK enrichment outside deepCrawl
   - Apollo-derived LinkedIn evidence outside deepCrawl
   - Crawl4AI LinkedIn attempt inside deepCrawl
5. Evidence cap is 36 (increased from 24)
6. Sitemap cache persisted in inputSnapshot
</verification>

<success_criteria>
Running research on a prospect collects evidence from sitemap pages, Google search mentions, KvK registry, and LinkedIn profile data. The evidence count is meaningfully higher than before Phase 17. All sources fail gracefully without blocking the pipeline.
</success_criteria>

<output>
After completion, create `.planning/phases/17-evidence-pipeline-enrichment/17-03-SUMMARY.md`
</output>
